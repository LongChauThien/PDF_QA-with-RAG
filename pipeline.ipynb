{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from io import BytesIO\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "# model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    # model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created.......\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"C:/Users/Asus/Documents/HK5/CS221/data/ver1/1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=450,chunk_overlap=200)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250,chunk_overlap=10)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=embeddings,collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "store = InMemoryStore()\n",
    "print(\"Vector Store Created.......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what is title of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"C:/Users/Asus/Documents/HK5/CS221/data/ver1/1.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "vector_store2 = Chroma.from_documents(texts, embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_retriever = vector_store2.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "retriever_ensemble = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever], weights=[0.25, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever_ensemble.get_relevant_documents(\"what is RAGAS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'C:/Users/Asus/Documents/HK5/CS221/data/ver1/1.pdf'}, page_content='in a faithful way, or the quality of the gener-\\nation itself. With RAGA S, we put forward a\\nsuite of metrics which can be used to evaluate\\nthese different dimensions without having to\\nrely on ground truth human annotations . We\\nposit that such a framework can crucially con-\\ntribute to faster evaluation cycles of RAG archi-\\ntectures, which is especially important given\\nthe fast adoption of LLMs.\\n1 Introduction'),\n",
       " Document(metadata={'source': 'C:/Users/Asus/Documents/HK5/CS221/data/ver1/1.pdf', 'page': 3}, page_content='ing candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context.\\nThe context relevance score is then computed as:\\nCR=number of extracted sentences\\ntotal number of sentences in c(q)(2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree'),\n",
       " Document(metadata={'source': 'C:/Users/Asus/Documents/HK5/CS221/data/ver1/1.pdf', 'page': 5}, page_content='Tran-Johnson, Scott Johnston, Sheer El Showk, Andy\\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\nBen Mann, Sam McCandlish, Chris Olah, and JaredKaplan. 2022. Language models (mostly) know what')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_return = [\n",
    "            {\n",
    "                'metadata': doc.metadata,\n",
    "                'page_content': doc.page_content\n",
    "            }\n",
    "            for doc in docs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'page': 0,\n",
       "   'source': './backend/app/upload_files/Identification_of_Gender_from_Facial_Features.pdf'},\n",
       "  'page_content': 'Copyright: ©  the author(s), publisher and licensee Technoscience Academy. This is an open -access article distributed under the \\nterms of the Creative Commons Attribution Non -Commercial License, which permits unrestricted non -commercial use, \\ndistribution, and reproducti on in any medium, provided the original work is properly cited  \\n International Journal of Scientific Research in Computer Science, Engineering and Information Technology'},\n",
       " {'metadata': {'page': 4,\n",
       "   'source': './backend/app/upload_files/Identification_of_Gender_from_Facial_Features.pdf'},\n",
       "  'page_content': 'difference  of the second variable onto the first \\nvariable by translation and rotation of original axes \\nand projecting data onto new axes. The direction of \\nprojection is decided using eige nvalues and \\neigenvectors. So, the first few modified  features \\n(termed as Principal Components) are rich in \\ninformation, whereas the last features contain mostly \\nnoise with negligible information in them. This'},\n",
       " {'metadata': {'source': './backend/app/upload_files/Identification_of_Gender_from_Facial_Features.pdf',\n",
       "   'page': 5},\n",
       "  'page_content': \"step.  \\n \\nChallenges  \\n \\nOne of the main challenges for classifying gender \\nusing  facial images is that the eff ect of the posture of \\nthe person, illumination and ground noise . While \\nneural networks are ready to learn representations, \\nthey're subject to certain spatial conditions of the \\ninput images. With the assistance of preprocessing, \\nthe proposed approach is in  a position to form all \\ninput images uniform. This helps in reliable\"},\n",
       " {'metadata': {'source': './backend/app/upload_files/Identification_of_Gender_from_Facial_Features.pdf',\n",
       "   'page': 5},\n",
       "  'page_content': 'score we can understand the app select that model or \\nnot.at next we will use ROC and AUC but for that we \\nrequire probability. To overcome this drawback we \\nhave to set Probability=True in our model evolution  \\nstep.  \\n \\nChallenges  \\n \\nOne of the main challenges for classifying gender \\nusing  facial images is that the eff ect of the posture of \\nthe person, illumination and ground noise . While'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: RAGAS refers to a suite of metrics proposed for evaluating different dimensions, such as context relevance, of question-answering systems. The metrics don't rely on human annotations, which can contribute to faster evaluation cycles for Reinforcement Learning with Human Feedback (RLHF) architectures, especially considering the fast adoption of Large Language Models (LLMs).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "try:\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever_ensemble, return_source_documents=True, chain_type_kwargs=chain_type_kwargs, verbose=True)\n",
    "    \n",
    "    def get_response(input):\n",
    "        query = input\n",
    "        response = qa(query)\n",
    "        # docs = vector_store.similarity_search_with_score(query)\n",
    "        result = 'Answer:' + response['result']\n",
    "        # +'\\n' + 'Similarity_score: '+ str(docs[0][1])\n",
    "        return result\n",
    "\n",
    "    print(get_response(\"what is RAGAS?\"))\n",
    "\n",
    "except ConnectionRefusedError as e:\n",
    "    print(f\"ConnectionRefusedError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're thinking of the great mountaineering achievement!\n",
      "\n",
      "The first person to reach the summit of Mount Everest was Tenzing Norgay, a Nepali Sherpa mountaineer, along with Sir Edmund Hillary from New Zealand. They successfully climbed to the top of the mountain on May 29, 1953. This historic moment marked a major milestone in mountaineering history!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You're thinking of the great mountaineering achievement!\\n\\nThe first person to reach the summit of Mount Everest was Tenzing Norgay, a Nepali Sherpa mountaineer, along with Sir Edmund Hillary from New Zealand. They successfully climbed to the top of the mountain on May 29, 1953. This historic moment marked a major milestone in mountaineering history!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "llm(\"The first man on the summit of Mount Everest, the highest peak on Earth, was ...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
